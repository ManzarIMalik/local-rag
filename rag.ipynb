{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import textwrap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232 pages of documentss loaded\n"
     ]
    }
   ],
   "source": [
    "loader = PyMuPDFLoader('./data/Szeliski_CVAABook_2ndEd.pdf')\n",
    "documents = loader.load()\n",
    "print(f\"{len(documents)} pages of documentss loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1238 chunks from 1232 pages\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1200, # removed to chunk one page at a time\n",
    "    # chunk_overlap=200,\n",
    "    )\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'Created {len(docs)} chunks from {len(documents)} pages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6 Geometric transformations\n",
      "177\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 3.50\n",
      "Line-based image warping (Beier and Neely 1992) © 1992 ACM: (a) distance\n",
      "computation and position transfer; (b) rendering algorithm; (c) two intermediate warps used\n",
      "for morphing.\n",
      "segment correspondence speciﬁes a translation, rotation, and scaling, i.e., a similarity trans-\n",
      "form (Table 3.3), for pixels in its vicinity, as shown in Figure 3.50a. Line segments inﬂuence\n",
      "the overall displacement of the image using a weighting function that depends on the mini-\n",
      "mum distance to the line segment (v in Figure 3.50a if u ∈[0, 1], else the shorter of the two\n",
      "distances to P and Q).\n",
      "One ﬁnal possibility for specifying displacement ﬁelds is to use a mesh speciﬁcally\n",
      "adapted to the underlying image content, as shown in Figure 3.49d. Specifying such meshes\n",
      "by hand can involve a fair amount of work; Gomes, Darsa et al. (1999) describe an interactive\n",
      "system for doing this. Once the two meshes have been speciﬁed, intermediate warps can be\n",
      "generated using linear interpolation and the displacements at mesh nodes can be interpolated\n",
      "using splines.\n",
      "3.6.3\n",
      "Application: Feature-based morphing\n",
      "While warps can be used to change the appearance of or to animate a single image, even\n",
      "more powerful effects can be obtained by warping and blending two or more images using\n",
      "a process now commonly known as morphing (Beier and Neely 1992; Lee, Wolberg et al.\n",
      "1996; Gomes, Darsa et al. 1999).\n",
      "Figure 3.51 shows the essence of image morphing. Instead of simply cross-dissolving\n",
      "between two images, which leads to ghosting as shown in the top row, each image is warped\n",
      "toward the other image before blending, as shown in the bottom row. If the correspondences\n",
      "have been set up well (using any of the techniques shown in Figure 3.49), corresponding\n",
      "features are aligned and no ghosting results.\n",
      "The above process is repeated for each intermediate frame being generated during a\n",
      "morph, using different blends (and amounts of deformation) at each interval. Let t ∈[0, 1] be\n",
      "the time parameter that describes the sequence of interpolated frames. The weighting func-\n"
     ]
    }
   ],
   "source": [
    "# print(docs[0].metadata)\n",
    "print(docs[200].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create a Chroma database\n",
    "# db = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"szeliski_cv\",\n",
    "#     persist_directory=\"./chroma_db\"\n",
    "#     )\n",
    "\n",
    "db = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings)\n",
    "\n",
    "db.save_local('./faiss_db/', index_name='szeliski_cv')\n",
    "# db.load_local('./faiss_db/', embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7ffabe66a920>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.load_local('./faiss_db/', embeddings=embeddings, index_name='szeliski_cv', allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model = \"llama3\",\n",
    "    # model = \"phi3\",\n",
    "    # model = \"gemma:2b\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You need either to explain the concept or answer the question about Computer Vision. \n",
    "Be detailed, use simple words and examples in your explanations. If required, utilize the relevant information.\n",
    "Also give source of information, along with page number which relates to retrieved content.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='158\\nComputer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\\nspace:\\n−\\n=\\nfrequency:\\n−\\n=\\nlow-pass\\nlower-pass\\nFigure 3.34\\nThe difference of two low-pass ﬁlters results in a band-pass ﬁlter. The dashed\\nblue lines show the close ﬁt to a half-octave Laplacian of Gaussian.\\nthe output of the L box directly to the subtraction in Figure 3.33). This variant has less\\naliasing, since it avoids one downsampling and upsampling round-trip, but it is not self-\\ninverting, since the Laplacian images are no longer adequate to reproduce the original image.\\nAs with the Gaussian pyramid, the term Laplacian is a bit of a misnomer, since their\\nband-pass images are really differences of (approximate) Gaussians, or DoGs,\\nDoG{I; σ1, σ2} = Gσ1 ∗I −Gσ2 ∗I = (Gσ1 −Gσ2) ∗I.\\n(3.70)\\nA Laplacian of Gaussian (which we saw in (3.26)) is actually its second derivative,\\nLoG{I; σ} = ∇2(Gσ ∗I) = (∇2Gσ) ∗I,\\n(3.71)\\nwhere\\n∇2 = ∂2\\n∂x2 + ∂2\\n∂y2\\n(3.72)\\nis the Laplacian (operator) of a function. Figure 3.34 shows how the Differences of Gaussian\\nand Laplacians of Gaussian look in both space and frequency.\\nLaplacians of Gaussian have elegant mathematical properties, which have been widely\\nstudied in the scale-space community (Witkin 1983; Witkin, Terzopoulos, and Kass 1986;\\nLindeberg 1990; Nielsen, Florack, and Deriche 1997) and can be used for a variety of appli-\\ncations including edge detection (Marr and Hildreth 1980; Perona and Malik 1990b), stereo\\nmatching (Witkin, Terzopoulos, and Kass 1987), and image enhancement (Nielsen, Florack,\\nand Deriche 1997).\\nOne particularly useful application of the Laplacian pyramid is in the manipulation of\\nlocal contrast as well as the tone mapping of high dynamic range images (Section 10.2.1).\\nParis, Hasinoff, and Kautz (2011) present a technique they call local Laplacian ﬁlters, which\\nuses local range clipping in the construction of a modiﬁed Laplacian pyramid, as well as', metadata={'source': './data/Szeliski_CVAABook_2ndEd.pdf', 'file_path': './data/Szeliski_CVAABook_2ndEd.pdf', 'page': 183, 'total_pages': 1232, 'format': 'PDF 1.6', 'title': 'Computer Vision: Algorithms and Applications, 2nd Edition', 'author': 'Richard Szeliski', 'subject': 'Computer Vision', 'keywords': 'computer vision', 'creator': 'LaTeX with hyperref', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20220129111642-08'00'\", 'modDate': \"D:20220129111642-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='166\\nComputer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n(g)\\n(h)\\nFigure 3.41\\nLaplacian pyramid blending (Burt and Adelson 1983b) © 1983 ACM: (a)\\noriginal image of apple, (b) original image of orange, (c) regular splice, (d) pyramid blend.\\nA masked blend of two images: (e) ﬁrst input image, (f) second input image, (g) region mask,\\n(h) blended image.', metadata={'source': './data/Szeliski_CVAABook_2ndEd.pdf', 'file_path': './data/Szeliski_CVAABook_2ndEd.pdf', 'page': 191, 'total_pages': 1232, 'format': 'PDF 1.6', 'title': 'Computer Vision: Algorithms and Applications, 2nd Edition', 'author': 'Richard Szeliski', 'subject': 'Computer Vision', 'keywords': 'computer vision', 'creator': 'LaTeX with hyperref', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20220129111642-08'00'\", 'modDate': \"D:20220129111642-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing MMR search\n",
    "question = \"What is Laplacian pyramid blending?\"\n",
    "db.max_marginal_relevance_search(question, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['title'].split('/')[-1][:-4]\n",
    "            + ' - page: '\n",
    "            + str(source.metadata['page']+1)\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    \n",
    "    llm_response = qa_chain.invoke(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, image stitching is normally used to composite partially overlapping photographs by:\n",
      "\n",
      "1. Stitching the background scene to create a single sprite image that can be transmitted and used to re-create the background in each frame (Figure 8.7).\n",
      "2. Removing moving foreground objects using median filtering or extracting them into a separate layer and later composing them back into the stitched panoramas.\n",
      "3. Creating animated panoramic video textures by animating different portions of a panoramic scene with independently moving video loops.\n",
      "\n",
      "These techniques are used to summarize and compress videos taken with a panning camera, allowing for efficient transmission and storage of large amounts of visual data.\n",
      "\n",
      "Sources: \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 1080 \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 548 \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 574\n",
      "\n",
      "Time elapsed: 85 s\n"
     ]
    }
   ],
   "source": [
    "query = \"How image stitching is normally used to composite partially overlapping photographs\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category recognition in the bag of words (also known as bag of features or bag of keypoints) approach is a simple algorithm for recognizing categories of objects or images. It represents objects and images as unordered collections of feature descriptors, also known as visual words.\n",
      "\n",
      "In this approach, the distribution (histogram) of visual words found in the query image is computed and compared to those found in the training images. This is done by extracting features at keypoints and then quantizing them to get a distribution over the learned visual words (feature cluster centers). The feature distribution histogram is used to learn a decision surface using a classification algorithm, such as a support vector machine.\n",
      "\n",
      "The bag of words approach does not require geometric verification, unlike instance recognition, since individual instances of generic visual categories have relatively little spatial coherence to their features. This makes it a simple and efficient way to recognize categories of objects or images.\n",
      "\n",
      "Sources: \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 436 \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 378 \n",
      "Computer Vision: Algorithms and Applications, 2nd Edi - page: 475\n",
      "\n",
      "Time elapsed: 72 s\n"
     ]
    }
   ],
   "source": [
    "query = \"What is category recognition in the bag of words (also known as bag of features or bag of keypoints)?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = llm\n",
    "critic_llm = llm\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
